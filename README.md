---

# ğŸš€ MLOps Demo: Model Deployment with Docker & MLflow

Welcome to the **MLOps Demo** â€” a hands-on project showcasing how to deploy a Machine Learning model using **Docker** and **MLflow**, complete with a REST API for predictions. This setup ensures seamless tracking, reproducibility, and portability, essential pillars of **MLOps**.

---

## ğŸ“Œ Project Overview

This project demonstrates:

* âœ… **Model Tracking & Packaging** with MLflow
* ğŸ³ **Containerized Deployment** using Docker
* ğŸŒ **Serving the Model via REST API** (FastAPI/Gunicorn)
* ğŸ§ª Health check & prediction endpoints
* ğŸ“ Persistent logging using mounted volumes

---

## ğŸ› ï¸ Tech Stack

| Tool     | Role                                         |
| -------- | -------------------------------------------- |
| Python   | Core programming language                    |
| MLflow   | Model tracking, packaging, and deployment    |
| Docker   | Containerization for reproducibility         |
| Gunicorn | Production-ready WSGI server for Python apps |
| FastAPI  | Lightweight and fast web framework           |
| SQLite   | Lightweight database for MLflow backend      |

---

## ğŸ“‚ Directory Structure

```
mlops-demo/
â”‚
â”œâ”€â”€ app.py                   # REST API logic (FastAPI/Gunicorn)
â”œâ”€â”€ inference.py             # Model loading and prediction
â”œâ”€â”€ train.py                 # Model training and MLflow logging
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ Dockerfile               # Container setup
â”œâ”€â”€ mlruns/                  # MLflow tracking folder (mounted)
â””â”€â”€ mlflow.db                # SQLite backend store (mounted)
```

---

## ğŸ§ª Endpoints

Once deployed, the API provides:

* `GET /` â†’ API metadata and status
* `GET /health` â†’ Health check endpoint
* `POST /predict` â†’ Make predictions with your model

---

## ğŸš€ How to Run

### 1. ğŸ—ï¸ Build Docker Image

```bash
docker build -t mlops-demo .
```

### 2. ğŸ§³ Run Docker Container with Volumes

```bash
docker run -p 5001:5000 \
  -v "C:/your/path/mlruns:/app/mlruns" \
  -v "C:/your/path/mlflow.db:/app/mlflow.db" \
  mlops-demo
```

### 3. ğŸŒ Access the API

Visit: [http://localhost:5001](http://localhost:5001)

Use tools like **Postman** or **cURL** to test the `/predict` endpoint.

---

## ğŸ“ˆ Sample Output (on visiting `/`)

```json
{
  "api_version": "1.0",
  "endpoints": {
    "/": "API information (this message)",
    "/health": "Health check endpoint",
    "/predict": "Prediction endpoint (POST)"
  },
  "model_loaded": true,
  "status": "online"
}
```

---

## ğŸ¥ Suggested Demo Topics

You can showcase this as a part of:

* ğŸ”¹ **Model Deployment in MLOps**
* ğŸ”¹ **MLflow + Docker Integration**
* ğŸ”¹ **End-to-End ML Inference Pipeline**
* ğŸ”¹ **API-based Model Serving**

---

## ğŸ™Œ Credits

Built by **Pallav Sharma** as a demonstration of **real-world MLOps practices**.

---

